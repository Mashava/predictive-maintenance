{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance in Jet Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh oh! There can be no more important application of predictive maintenance than for jet engines. For you to have peace when flying, those engines better not be failing anytime soon. Predictive maintenance tries to balance the cost of failures against the cost of maintenance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [The Use Case](#The-Use-Case)\n",
    "- [Success Criteria](#Success-Criteria)\n",
    "- [AI Solutions](#AI-Solutions)\n",
    "    - [LSTM](#LSTM)\n",
    "    - [Denoising](#Denoising)\n",
    "- [Thoughts](#Thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an airline company does daily maintenance of their plane engines they will soon go bankrupt. Same applies if they never do any maintenance. Their planes may cause fatal accidents result in them going out of business. So the question is, what is the right amout of scheduled maintenance? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimize](images/optimize.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to predict Remaining Useful Life (RUL) enables the optimization of maintenance costs. However, this requires a robust IoT strategy to collect indicators of failure from the jet engines. Even with good data collection strategy, events like engine failures rarely occur. Simulations can help generate training data as what [NASA did](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A success predictive maintenance model should be able to operate at the optimal point of the trade-off between the maintenance costs and downtime costs. Since the model will be predicting RUL, metrics to optimize are the Mean Squared Error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonsymetric](images/nonsymetric.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 9)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF Version: \", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os[n] is the n-th operating setting whilst ms[n] is the n-th sensor measurement\n",
    "\n",
    "names = [\"unit\", \"time\",\"os1\", \"os2\", \"os3\"] + [\"ms{}\".format(ms) for ms in range(1,22)]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input are 4 files. File 1 and 3 have simulations done at sea level conditions while file 2 and 4 have non-sea level conditions. File 1 and 2 experience the HPC degradation fault mode whilst file 3 and 4 experience 2 fault modes of HPC degradation and fan degradation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe1 = pd.read_csv(\"data/train_FD001.txt\", sep=\" \", names=names, index_col=False) \n",
    "train_dataframe2 = pd.read_csv(\"data/train_FD002.txt\", sep=\" \", names=names, index_col=False) \n",
    "train_dataframe3 = pd.read_csv(\"data/train_FD003.txt\", sep=\" \", names=names, index_col=False) \n",
    "train_dataframe4 = pd.read_csv(\"data/train_FD004.txt\", sep=\" \", names=names, index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe1 = pd.read_csv(\"data/test_FD001.txt\", sep=\" \", names=names, index_col=False) \n",
    "test_dataframe2 = pd.read_csv(\"data/test_FD002.txt\", sep=\" \", names=names, index_col=False) \n",
    "test_dataframe3 = pd.read_csv(\"data/test_FD003.txt\", sep=\" \", names=names, index_col=False) \n",
    "test_dataframe4 = pd.read_csv(\"data/test_FD004.txt\", sep=\" \", names=names, index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label1 = pd.read_csv(\"data/RUL_FD001.txt\", sep=\" \", names=['rul'], index_col=False) \n",
    "test_label2 = pd.read_csv(\"data/RUL_FD002.txt\", sep=\" \", names=['rul'], index_col=False) \n",
    "test_label3 = pd.read_csv(\"data/RUL_FD003.txt\", sep=\" \", names=['rul'], index_col=False) \n",
    "test_label4 = pd.read_csv(\"data/RUL_FD004.txt\", sep=\" \", names=['rul'], index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Each file corresponds to a different condition. ONE is for sea level and SIX is probably for about sea level\n",
    "train_dataframe1[\"conditions\"] = \"ONE\"\n",
    "train_dataframe2[\"conditions\"] = \"SIX\"\n",
    "train_dataframe3[\"conditions\"] = \"ONE\"\n",
    "train_dataframe4[\"conditions\"] = \"SIX\"\n",
    "\n",
    "test_dataframe1[\"conditions\"] = \"ONE\"\n",
    "test_dataframe2[\"conditions\"] = \"SIX\"\n",
    "test_dataframe3[\"conditions\"] = \"ONE\"\n",
    "test_dataframe4[\"conditions\"] = \"SIX\"\n",
    "\n",
    "test_label1[\"conditions\"] = \"ONE\"\n",
    "test_label2[\"conditions\"] = \"SIX\"\n",
    "test_label3[\"conditions\"] = \"ONE\"\n",
    "test_label4[\"conditions\"] = \"SIX\"\n",
    "\n",
    "#The failure modes are difficult to have an attribute for as file 3 and 4 have 2 failure modes\n",
    "train_dataframe1[\"file\"] = 1\n",
    "train_dataframe2[\"file\"] = 2\n",
    "train_dataframe3[\"file\"] = 3\n",
    "train_dataframe4[\"file\"] = 4\n",
    "\n",
    "test_dataframe1[\"file\"] = 1\n",
    "test_dataframe2[\"file\"] = 2\n",
    "test_dataframe3[\"file\"] = 3\n",
    "test_dataframe4[\"file\"] = 4\n",
    "\n",
    "test_label1[\"file\"] = 1\n",
    "test_label2[\"file\"] = 2\n",
    "test_label3[\"file\"] = 3\n",
    "test_label4[\"file\"] = 4\n",
    "\n",
    "train_df = pd.concat([train_dataframe1, train_dataframe2, train_dataframe3, train_dataframe4])\n",
    "test_df = pd.concat([test_dataframe1, test_dataframe2, test_dataframe3, test_dataframe4])\n",
    "label_df = pd.concat([test_label1, test_label2, test_label3, test_label4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_by_unit = train_df.groupby(['file', 'unit']).size().reset_index(name='measurements')\n",
    "measurements_by_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = train_df.groupby(['file'])['unit'].nunique().reset_index(name='units')\n",
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_measurements_by_unit = measurements_by_unit.groupby(['file'])['measurements'].min().reset_index(name='min_measurements')\n",
    "min_measurements_by_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like as the machine approaches failure measurements like ms2, ms3, ms4, ms8, ms9, ms11, ms14, ms15 and ms16 start to increase and ms7, ms12, ms20 and ms21 start to decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['os1', 'os2', 'os3', 'ms1', 'ms2', 'ms3', 'ms4', 'ms5', 'ms6','ms7','ms8', \n",
    "  'ms9', 'ms10', 'ms11', 'ms12', 'ms13', 'ms14', 'ms15', 'ms16', 'ms17', 'ms18', 'ms19', 'ms20', 'ms21']\n",
    "plot_features = train_df[(train_df['unit']==3) & (train_df['file']==1)][plot_cols]\n",
    "plot_features.index = train_df[(train_df['unit']==3) & (train_df['file']==1)]['time']\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features = train_df[(train_df['unit']==3) & (train_df['file']==4)][plot_cols]\n",
    "plot_features.index = train_df[(train_df['unit']==3) & (train_df['file']==4)]['time']\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For files 2 and 4 where the conditions are not sea level, it looks like the failing pattern is not apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features = train_df[(train_df['unit']==3) & (train_df['file']==2)][plot_cols]\n",
    "plot_features.index = train_df[(train_df['unit']==3) & (train_df['file']==2)]['time']\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features = train_df[(train_df['unit']==3) & (train_df['file']==3)][plot_cols]\n",
    "plot_features.index = train_df[(train_df['unit']==3) & (train_df['file']==3)]['time']\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df['file']==1)].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df['file']==2)].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df['file']==3)].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df['file']==4)].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "from pandas import read_csv\n",
    "import pandas\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "seed = 7\n",
    "batch_size = 100\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "train_dataframe = read_csv(\"data/train_FD001.txt\", sep=\" \", header=0, engine='python') \n",
    "\n",
    "test_dataframe_x = read_csv(\"data/test_FD001.txt\", sep=\" \", header=0, engine='python') \n",
    "test_dataframe_y = read_csv(\"data/RUL_FD001.txt\", sep=\" \", header=0, engine='python')\n",
    "\n",
    "columns = train_dataframe.columns\n",
    "train_dataset = train_dataframe.values\n",
    "test_dataset_x = test_dataframe_x.values\n",
    "test_dataset_y = test_dataframe_y.values\n",
    "\n",
    "\n",
    "input_dimensions = train_dataset.shape[1]\n",
    "print(input_dimensions)\n",
    "\n",
    "train_x = train_dataset[:,0:input_dimensions-1]\n",
    "train_y = train_dataset[:,input_dimensions-1]\n",
    "\n",
    "test_x = test_dataset_x\n",
    "test_y = test_dataset_y[:,0]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "scaler_x = MinMaxScaler(feature_range=(0, 1)).fit(train_x)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1)).fit(train_y)\n",
    "train_x = scaler_x.transform(train_x)\n",
    "train_y = scaler_y.transform(train_y)\n",
    "test_x = scaler_x.transform(test_x)\n",
    "test_y = scaler_y.transform(test_y)\n",
    "\n",
    "print(train_x.shape)\n",
    "\n",
    "\n",
    "#develop autoencoder model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dimensions-1, input_dim=input_dimensions-1, kernel_initializer='random_normal', activation='sigmoid'))\n",
    "model.add(Dense(4, kernel_initializer='random_normal', activation='sigmoid'))\n",
    "model.add(Dense(input_dimensions-1))\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.01, momentum=0.1, decay=0.0, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_x, epochs=200, batch_size=batch_size, verbose=2, shuffle=True)\n",
    "\n",
    "train_predictions = model.predict(train_x)\n",
    "pandas.DataFrame(train_predictions, columns = columns[0:input_dimensions-1]).to_csv(path_or_buf='data/autoencoded.csv')\n",
    "\n",
    "\n",
    "#develop predictive model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=input_dimensions-1, kernel_initializer='random_normal', activation='relu'))\n",
    "model.add(Dense(128, kernel_initializer='random_normal', activation='relu'))\n",
    "model.add(Dense(32, kernel_initializer='random_normal', activation='relu'))\n",
    "model.add(Dense(16, kernel_initializer='random_normal', activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "#use denoised values from autoencoder to train model\n",
    "model.fit(train_predictions, train_y, epochs=2000, batch_size=batch_size, verbose=2, shuffle=True)\n",
    "\n",
    "train_predictions = model.predict(train_x)\n",
    "test_predictions = model.predict(test_x)\n",
    "\n",
    "# invert predictions\n",
    "train_predictions = scaler_y.inverse_transform(train_predictions)\n",
    "train_y = scaler_y.inverse_transform([train_y])\n",
    "test_predictions = scaler_y.inverse_transform(test_predictions)\n",
    "test_y = scaler_y.inverse_transform([test_y])\n",
    "# calculate mean squared error\n",
    "\n",
    "train_score = math.sqrt(mean_squared_error(train_y[0], train_predictions[:,0]))\n",
    "print('Train Score: %.4f RMSE' % (train_score))\n",
    "test_rmse = math.sqrt(mean_squared_error(test_y[0], test_predictions[:,0]))\n",
    "print('Test Score: %.4f RMSE' % (test_rmse))\n",
    "test_mae = mean_absolute_error(test_y[0], test_predictions[:,0])\n",
    "print('Test Score: %.4f MAE' % (test_mae))\n",
    "\n",
    "print ('Mean yields actual: %.3f vs predicted: %.3f' % (numpy.mean(test_y[0]), numpy.mean(test_predictions[:,0])))\n",
    "test_actual_vs_predictions = pandas.concat([pandas.DataFrame(test_y[0], columns=['actual']).reset_index(drop=True), pandas.DataFrame(test_predictions[:,0], columns = ['predicted'])], axis=1)\n",
    "test_actual_vs_predictions.to_csv(path_or_buf='data/predicted.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
